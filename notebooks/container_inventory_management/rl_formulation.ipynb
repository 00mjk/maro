{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "This notebook demonstrates how to use MARO's reinforcement learning (RL) toolkit to solve the container inventory management ([CIM](https://maro.readthedocs.io/en/latest/scenarios/container_inventory_management.html)) problem. It is formalized as a multi-agent reinforcement learning problem, where each port acts as a decision agent. The agents take actions independently, e.g., loading containers to vessels or discharging containers from vessels.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [State Shaper](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#shapers)\n",
    "\n",
    "State shaper converts the environment observation to the model input state which includes temporal and spatial information. For this scenario, the model input state includes: \n",
    "\n",
    "- Temporal information, including the past week's information of ports and vessels, such as shortage on port and remaining space on vessel. \n",
    "\n",
    "- Spatial information, it including the related downstream port features.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from maro.rl import Shaper\n",
    "\n",
    "\n",
    "PORT_ATTRIBUTES = [\"empty\", \"full\", \"on_shipper\", \"on_consignee\", \"booking\", \"shortage\", \"fulfillment\"]\n",
    "VESSEL_ATTRIBUTES = [\"empty\", \"full\", \"remaining_space\"]\n",
    "\n",
    "\n",
    "class CIMStateShaper(Shaper):\n",
    "    def __init__(self, *, look_back, max_ports_downstream):\n",
    "        super().__init__()\n",
    "        self._look_back = look_back\n",
    "        self._max_ports_downstream = max_ports_downstream\n",
    "        self._dim = (look_back + 1) * (max_ports_downstream + 1) * len(PORT_ATTRIBUTES) + len(VESSEL_ATTRIBUTES)\n",
    "\n",
    "    def __call__(self, decision_event, snapshot_list):\n",
    "        tick, port_idx, vessel_idx = decision_event.tick, decision_event.port_idx, decision_event.vessel_idx\n",
    "        ticks = [tick - rt for rt in range(self._look_back - 1)]\n",
    "        future_port_idx_list = snapshot_list[\"vessels\"][tick: vessel_idx: 'future_stop_list'].astype('int')\n",
    "        port_features = snapshot_list[\"ports\"][ticks: [port_idx] + list(future_port_idx_list): PORT_ATTRIBUTES]\n",
    "        vessel_features = snapshot_list[\"vessels\"][tick: vessel_idx: VESSEL_ATTRIBUTES]\n",
    "        state = np.concatenate((port_features, vessel_features))\n",
    "        return state\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self._dim\n",
    "    \n",
    "# Create a state shaper\n",
    "state_shaper = CIMStateShaper(look_back=7, max_ports_downstream=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Action Shaper](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#shapers)\n",
    "\n",
    "Action shaper is used to convert an agent's model output to an environment executable action. For this specific scenario, the output is a discrete index that corresponds to a percentage indicating the fraction of containers to be loaded to or discharged from the arriving vessel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maro.simulator.scenarios.cim.common import Action, ActionType\n",
    "\n",
    "\n",
    "class CIMActionShaper(Shaper):\n",
    "    def __init__(self, action_space):\n",
    "        super().__init__()\n",
    "        self._action_space = action_space\n",
    "        self._zero_action_index = action_space.index(0)\n",
    "\n",
    "    def __call__(self, model_action, decision_event, snapshot_list):\n",
    "        scope = decision_event.action_scope\n",
    "        tick = decision_event.tick\n",
    "        port_idx = decision_event.port_idx\n",
    "        vessel_idx = decision_event.vessel_idx\n",
    "\n",
    "        port_empty = snapshot_list[\"ports\"][tick: port_idx: [\"empty\", \"full\", \"on_shipper\", \"on_consignee\"]][0]\n",
    "        vessel_remaining_space = snapshot_list[\"vessels\"][tick: vessel_idx: [\"empty\", \"full\", \"remaining_space\"]][2]\n",
    "        early_discharge = snapshot_list[\"vessels\"][tick:vessel_idx: \"early_discharge\"][0]\n",
    "        assert 0 <= model_action < len(self._action_space)\n",
    "        operation_num = self._action_space[model_action]\n",
    "\n",
    "        if model_action < self._zero_action_index:\n",
    "            actual_action = max(round(operation_num * port_empty), -vessel_remaining_space)\n",
    "            action_type = ActionType.LOAD\n",
    "        elif model_action > self._zero_action_index:\n",
    "            plan_action = operation_num * (scope.discharge + early_discharge) - early_discharge\n",
    "            actual_action = round(plan_action) if plan_action > 0 else round(operation_num * scope.discharge)\n",
    "            action_type = ActionType.DISCHARGE\n",
    "        else:\n",
    "            actual_action = 0\n",
    "            action_type = None\n",
    "\n",
    "        return Action(vessel_idx, port_idx, abs(actual_action), action_type)\n",
    "    \n",
    "# Create an action shaper\n",
    "NUM_ACTIONS = 21\n",
    "action_shaper = CIMActionShaper(action_space=list(np.linspace(-1.0, 1.0, NUM_ACTIONS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Experience Shaper](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#shapers)\n",
    "\n",
    "Experience shaper is used to convert an episode trajectory to trainable experiences for RL agents. For this specific scenario, the reward is a linear combination of fulfillment and shortage in a limited time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class CIMExperienceShaper(Shaper):\n",
    "    def __init__(\n",
    "        self, *, time_window: int, time_decay_factor: float, fulfillment_factor: float, shortage_factor: float\n",
    "    ):\n",
    "        super().__init__(reward_func=None)\n",
    "        self._time_window = time_window\n",
    "        self._time_decay_factor = time_decay_factor\n",
    "        self._fulfillment_factor = fulfillment_factor\n",
    "        self._shortage_factor = shortage_factor\n",
    "        self._trajectory = {key: [] for key in [\"state\", \"action\", \"agent_id\", \"event\"]}\n",
    "    \n",
    "    def __call__(self, snapshot_list):\n",
    "        states = self._trajectory[\"state\"]\n",
    "        actions = self._trajectory[\"action\"]\n",
    "        agent_ids = self._trajectory[\"agent_id\"]\n",
    "        events = self._trajectory[\"event\"]\n",
    "\n",
    "        experiences_by_agent = defaultdict(lambda: defaultdict(list))\n",
    "        for i in range(len(states) - 1):\n",
    "            experiences = experiences_by_agent[agent_ids[i]]\n",
    "            experiences[\"state\"].append(states[i])\n",
    "            experiences[\"action\"].append(actions[i])\n",
    "            experiences[\"reward\"].append(self._compute_reward(events[i], snapshot_list))\n",
    "            experiences[\"next_state\"].append(states[i + 1])\n",
    "\n",
    "        return dict(experiences_by_agent)\n",
    "\n",
    "    def record(self, transition: dict):\n",
    "        for key, val in transition.items():\n",
    "            self._trajectory[key].append(val)\n",
    "\n",
    "    def reset(self):\n",
    "        self._trajectory = {key: [] for key in [\"state\", \"action\", \"agent_id\", \"event\"]}\n",
    "\n",
    "    def _compute_reward(self, decision_event, snapshot_list):\n",
    "        start_tick = decision_event.tick + 1\n",
    "        end_tick = decision_event.tick + self._time_window\n",
    "        ticks = list(range(start_tick, end_tick))\n",
    "\n",
    "        # calculate tc reward\n",
    "        future_fulfillment = snapshot_list[\"ports\"][ticks::\"fulfillment\"]\n",
    "        future_shortage = snapshot_list[\"ports\"][ticks::\"shortage\"]\n",
    "        decay_list = [\n",
    "            self._time_decay_factor ** i for i in range(end_tick - start_tick)\n",
    "            for _ in range(future_fulfillment.shape[0] // (end_tick - start_tick))\n",
    "        ]\n",
    "\n",
    "        tot_fulfillment = np.dot(future_fulfillment, decay_list)\n",
    "        tot_shortage = np.dot(future_shortage, decay_list)\n",
    "\n",
    "        return np.float32(self._fulfillment_factor * tot_fulfillment - self._shortage_factor * tot_shortage)\n",
    "    \n",
    "# Create an experience shaper\n",
    "experience_shaper = CIMExperienceShaper(time_window=100, fulfillment_factor=1.0, shortage_factor=1.0, time_decay_factor=0.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Agent](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#agent)\n",
    "\n",
    "For this scenario, the agent is the algorithmic abstraction of a port. We choose DQN as our underlying learning algorithm with a TD-error-based sampling mechanism.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.functional import smooth_l1_loss\n",
    "from torch.optim import RMSprop\n",
    "\n",
    "from maro.rl import DQN, DQNConfig, FullyConnectedBlock, OptimOption, SimpleMultiHeadModel, SimpleStore\n",
    "from maro.utils import set_seeds\n",
    "\n",
    "\n",
    "def create_dqn_agents(agent_id_list):\n",
    "    set_seeds(64)  # for reproducibility\n",
    "    agent_dict = {}\n",
    "    for agent_id in agent_id_list:\n",
    "        q_net = FullyConnectedBlock(\n",
    "            input_dim=state_shaper.dim,\n",
    "            hidden_dims=[256, 128, 64],\n",
    "            output_dim=NUM_ACTIONS,\n",
    "            activation=nn.LeakyReLU,\n",
    "            is_head=True,\n",
    "            batch_norm=True, \n",
    "            softmax=False,\n",
    "            skip_connection=False,\n",
    "            dropout_p=.0\n",
    "        )\n",
    "        \n",
    "        learning_model = SimpleMultiHeadModel(\n",
    "            q_net, optim_option=OptimOption(optim_cls=RMSprop, optim_params={\"lr\": 0.05})\n",
    "        )\n",
    "        agent_dict[agent_id] = DQN(\n",
    "            agent_id, \n",
    "            learning_model, \n",
    "            config=DQNConfig(\n",
    "                reward_discount=.0, \n",
    "                min_exp_to_train=1024,\n",
    "                num_batches=10,\n",
    "                batch_size=128, \n",
    "                target_update_freq=5, \n",
    "                tau=0.1, \n",
    "                is_double=True, \n",
    "                per_sample_td_error=True,\n",
    "                loss_cls=nn.SmoothL1Loss\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return agent_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Actor](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#actor)\n",
    "\n",
    "The sole purpose of an actor is to perform roll-outs and collect experiences. An actor consists of an environment instance, an agent (a single agent or multiple agents wrapped by MultiAgentWrapper) and optionally a state shaper, an action shaper and an experience shaper if certain conversions are necessary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maro.rl import AbsActor\n",
    "\n",
    "\n",
    "class Actor(AbsActor):\n",
    "    def __init__(self, env, agent, state_shaper, action_shaper, experience_shaper):\n",
    "        super().__init__(\n",
    "            env, agent, \n",
    "            state_shaper=state_shaper, action_shaper=action_shaper, experience_shaper=experience_shaper\n",
    "        )\n",
    "\n",
    "    def roll_out(self, index, training=True):\n",
    "        self.env.reset()\n",
    "        metrics, event, is_done = self.env.step(None)\n",
    "        while not is_done:\n",
    "            state = self.state_shaper(event, self.env.snapshot_list)\n",
    "            agent_id = str(event.port_idx)\n",
    "            action = self.agent[agent_id].choose_action(state)\n",
    "            self.experience_shaper.record(\n",
    "                {\"state\": state, \"agent_id\": agent_id, \"event\": event, \"action\": action}\n",
    "            )\n",
    "            metrics, event, is_done = self.env.step(self.action_shaper(action, event, self.env.snapshot_list))\n",
    "\n",
    "        exp = self.experience_shaper(self.env.snapshot_list) if training else None\n",
    "        self.experience_shaper.reset()\n",
    "\n",
    "        return exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Learner](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#learner)\n",
    "\n",
    "A learner implements the main training loop and policy update logic. It contains an actor for performing roll-outs and collecting experiences and a scheduler for controlling the training loop and generating exploration parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maro.rl import AbsLearner\n",
    "from maro.utils import LogFormat, Logger\n",
    "\n",
    "\n",
    "class Learner(AbsLearner):\n",
    "    def __init__(self, actor, scheduler):\n",
    "        super().__init__(actor, scheduler)\n",
    "        self._logger = Logger(\"learner\", format_=LogFormat.simple, auto_timestamp=False)\n",
    "\n",
    "    def learn(self):\n",
    "        for exploration_params in self.scheduler:\n",
    "            # load exploration parameters\n",
    "            self.actor.agent.set_exploration_params(exploration_params)\n",
    "            exp = self.actor.roll_out(self.scheduler.iter)\n",
    "            self._logger.info(\n",
    "                f\"ep {self.scheduler.iter} - performance: {self.actor.env.metrics}, \"\n",
    "                f\"exploration_params: {exploration_params}\"\n",
    "            )\n",
    "            self.update(exp)\n",
    "\n",
    "    def update(self, experiences_by_agent):\n",
    "        # Store experiences for each agent\n",
    "        for agent_id, exp in experiences_by_agent.items():\n",
    "            exp.update({\"loss\": [1e8] * len(list(exp.values())[0])})\n",
    "            self.actor.agent[agent_id].store_experiences(exp)\n",
    "\n",
    "        for agent in self.actor.agent.agent_dict.values():\n",
    "            agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop\n",
    "\n",
    "This code cell demonstrates the typical workflow of a learning policy's interaction with a MARO environment. \n",
    "\n",
    "- Initialize an environment with specific scenario and topology parameters. \n",
    "\n",
    "- Define scenario-specific components, e.g. shapers. \n",
    "\n",
    "- Create agents. \n",
    "\n",
    "- Create an actor and a learner to start the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:15:32 | learner | INFO | ep 0 - performance: {'order_requirements': 2240000, 'container_shortage': 1352136, 'operation_number': 3254760}, exploration_params: {'epsilon': 0.4}\n",
      "10:15:37 | learner | INFO | ep 1 - performance: {'order_requirements': 2240000, 'container_shortage': 1249849, 'operation_number': 3426101}, exploration_params: {'epsilon': 0.39840000000000003}\n",
      "10:15:41 | learner | INFO | ep 2 - performance: {'order_requirements': 2240000, 'container_shortage': 1174857, 'operation_number': 3816050}, exploration_params: {'epsilon': 0.39680000000000004}\n",
      "10:15:46 | learner | INFO | ep 3 - performance: {'order_requirements': 2240000, 'container_shortage': 1168029, 'operation_number': 3783409}, exploration_params: {'epsilon': 0.39520000000000005}\n",
      "10:15:51 | learner | INFO | ep 4 - performance: {'order_requirements': 2240000, 'container_shortage': 1478014, 'operation_number': 3503012}, exploration_params: {'epsilon': 0.39360000000000006}\n",
      "10:15:56 | learner | INFO | ep 5 - performance: {'order_requirements': 2240000, 'container_shortage': 1450497, 'operation_number': 4235336.0}, exploration_params: {'epsilon': 0.39200000000000007}\n",
      "10:16:01 | learner | INFO | ep 6 - performance: {'order_requirements': 2240000, 'container_shortage': 1250538, 'operation_number': 3681417}, exploration_params: {'epsilon': 0.3904000000000001}\n",
      "10:16:06 | learner | INFO | ep 7 - performance: {'order_requirements': 2240000, 'container_shortage': 1923270, 'operation_number': 2793886}, exploration_params: {'epsilon': 0.3888000000000001}\n",
      "10:16:11 | learner | INFO | ep 8 - performance: {'order_requirements': 2240000, 'container_shortage': 1621199, 'operation_number': 2655534}, exploration_params: {'epsilon': 0.3872000000000001}\n",
      "10:16:17 | learner | INFO | ep 9 - performance: {'order_requirements': 2240000, 'container_shortage': 1185373, 'operation_number': 3303864}, exploration_params: {'epsilon': 0.3856000000000001}\n",
      "10:16:22 | learner | INFO | ep 10 - performance: {'order_requirements': 2240000, 'container_shortage': 720509, 'operation_number': 3923254}, exploration_params: {'epsilon': 0.3840000000000001}\n",
      "10:16:27 | learner | INFO | ep 11 - performance: {'order_requirements': 2240000, 'container_shortage': 604046, 'operation_number': 4055816}, exploration_params: {'epsilon': 0.38240000000000013}\n",
      "10:16:32 | learner | INFO | ep 12 - performance: {'order_requirements': 2240000, 'container_shortage': 721395, 'operation_number': 4088165}, exploration_params: {'epsilon': 0.38080000000000014}\n",
      "10:16:37 | learner | INFO | ep 13 - performance: {'order_requirements': 2240000, 'container_shortage': 569133, 'operation_number': 5024948}, exploration_params: {'epsilon': 0.37920000000000015}\n",
      "10:16:42 | learner | INFO | ep 14 - performance: {'order_requirements': 2240000, 'container_shortage': 672845, 'operation_number': 4619933}, exploration_params: {'epsilon': 0.37760000000000016}\n",
      "10:16:48 | learner | INFO | ep 15 - performance: {'order_requirements': 2240000, 'container_shortage': 899736, 'operation_number': 4688569}, exploration_params: {'epsilon': 0.37600000000000017}\n",
      "10:16:53 | learner | INFO | ep 16 - performance: {'order_requirements': 2240000, 'container_shortage': 950907, 'operation_number': 4453843}, exploration_params: {'epsilon': 0.3744000000000002}\n",
      "10:16:58 | learner | INFO | ep 17 - performance: {'order_requirements': 2240000, 'container_shortage': 642622, 'operation_number': 5087980}, exploration_params: {'epsilon': 0.3728000000000002}\n",
      "10:17:03 | learner | INFO | ep 18 - performance: {'order_requirements': 2240000, 'container_shortage': 804926, 'operation_number': 4627842}, exploration_params: {'epsilon': 0.3712000000000002}\n",
      "10:17:08 | learner | INFO | ep 19 - performance: {'order_requirements': 2240000, 'container_shortage': 955385, 'operation_number': 4955868}, exploration_params: {'epsilon': 0.3696000000000002}\n",
      "10:17:14 | learner | INFO | ep 20 - performance: {'order_requirements': 2240000, 'container_shortage': 829930, 'operation_number': 5148070}, exploration_params: {'epsilon': 0.3680000000000002}\n",
      "10:17:19 | learner | INFO | ep 21 - performance: {'order_requirements': 2240000, 'container_shortage': 644572, 'operation_number': 5105741}, exploration_params: {'epsilon': 0.3664000000000002}\n",
      "10:17:24 | learner | INFO | ep 22 - performance: {'order_requirements': 2240000, 'container_shortage': 555530, 'operation_number': 4839572}, exploration_params: {'epsilon': 0.36480000000000024}\n",
      "10:17:29 | learner | INFO | ep 23 - performance: {'order_requirements': 2240000, 'container_shortage': 1031378, 'operation_number': 3728440}, exploration_params: {'epsilon': 0.36320000000000024}\n",
      "10:17:35 | learner | INFO | ep 24 - performance: {'order_requirements': 2240000, 'container_shortage': 723926, 'operation_number': 5235602}, exploration_params: {'epsilon': 0.36160000000000025}\n",
      "10:17:41 | learner | INFO | ep 25 - performance: {'order_requirements': 2240000, 'container_shortage': 676156, 'operation_number': 5142291}, exploration_params: {'epsilon': 0.36000000000000026}\n",
      "10:17:46 | learner | INFO | ep 26 - performance: {'order_requirements': 2240000, 'container_shortage': 842840, 'operation_number': 5028770}, exploration_params: {'epsilon': 0.3584000000000003}\n",
      "10:17:52 | learner | INFO | ep 27 - performance: {'order_requirements': 2240000, 'container_shortage': 865620, 'operation_number': 4766610}, exploration_params: {'epsilon': 0.3568000000000003}\n",
      "10:17:58 | learner | INFO | ep 28 - performance: {'order_requirements': 2240000, 'container_shortage': 794776, 'operation_number': 5052284}, exploration_params: {'epsilon': 0.3552000000000003}\n",
      "10:18:04 | learner | INFO | ep 29 - performance: {'order_requirements': 2240000, 'container_shortage': 699853, 'operation_number': 5152245}, exploration_params: {'epsilon': 0.3536000000000003}\n",
      "10:18:10 | learner | INFO | ep 30 - performance: {'order_requirements': 2240000, 'container_shortage': 616300, 'operation_number': 4678601}, exploration_params: {'epsilon': 0.3520000000000003}\n",
      "10:18:16 | learner | INFO | ep 31 - performance: {'order_requirements': 2240000, 'container_shortage': 728820, 'operation_number': 4974799}, exploration_params: {'epsilon': 0.3504000000000003}\n",
      "10:18:22 | learner | INFO | ep 32 - performance: {'order_requirements': 2240000, 'container_shortage': 727185, 'operation_number': 4755672}, exploration_params: {'epsilon': 0.34880000000000033}\n",
      "10:18:28 | learner | INFO | ep 33 - performance: {'order_requirements': 2240000, 'container_shortage': 727381, 'operation_number': 4873634}, exploration_params: {'epsilon': 0.34720000000000034}\n",
      "10:18:34 | learner | INFO | ep 34 - performance: {'order_requirements': 2240000, 'container_shortage': 679647, 'operation_number': 4624782}, exploration_params: {'epsilon': 0.34560000000000035}\n",
      "10:18:40 | learner | INFO | ep 35 - performance: {'order_requirements': 2240000, 'container_shortage': 625849, 'operation_number': 4947661}, exploration_params: {'epsilon': 0.34400000000000036}\n",
      "10:18:46 | learner | INFO | ep 36 - performance: {'order_requirements': 2240000, 'container_shortage': 706626, 'operation_number': 4546677}, exploration_params: {'epsilon': 0.34240000000000037}\n",
      "10:18:51 | learner | INFO | ep 37 - performance: {'order_requirements': 2240000, 'container_shortage': 560302, 'operation_number': 4578177}, exploration_params: {'epsilon': 0.3408000000000004}\n",
      "10:18:57 | learner | INFO | ep 38 - performance: {'order_requirements': 2240000, 'container_shortage': 774175, 'operation_number': 5005750}, exploration_params: {'epsilon': 0.3392000000000004}\n",
      "10:19:03 | learner | INFO | ep 39 - performance: {'order_requirements': 2240000, 'container_shortage': 584020, 'operation_number': 4787339}, exploration_params: {'epsilon': 0.3376000000000004}\n",
      "10:19:08 | learner | INFO | ep 40 - performance: {'order_requirements': 2240000, 'container_shortage': 576394, 'operation_number': 4720419}, exploration_params: {'epsilon': 0.3360000000000004}\n",
      "10:19:14 | learner | INFO | ep 41 - performance: {'order_requirements': 2240000, 'container_shortage': 702115, 'operation_number': 4928421}, exploration_params: {'epsilon': 0.3344000000000004}\n",
      "10:19:19 | learner | INFO | ep 42 - performance: {'order_requirements': 2240000, 'container_shortage': 822003, 'operation_number': 5127494}, exploration_params: {'epsilon': 0.33280000000000043}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:19:25 | learner | INFO | ep 43 - performance: {'order_requirements': 2240000, 'container_shortage': 676856, 'operation_number': 4361711}, exploration_params: {'epsilon': 0.33120000000000044}\n",
      "10:19:31 | learner | INFO | ep 44 - performance: {'order_requirements': 2240000, 'container_shortage': 669537, 'operation_number': 5291246}, exploration_params: {'epsilon': 0.32960000000000045}\n",
      "10:19:37 | learner | INFO | ep 45 - performance: {'order_requirements': 2240000, 'container_shortage': 569000, 'operation_number': 4652232}, exploration_params: {'epsilon': 0.32800000000000046}\n",
      "10:19:43 | learner | INFO | ep 46 - performance: {'order_requirements': 2240000, 'container_shortage': 604969, 'operation_number': 5123438}, exploration_params: {'epsilon': 0.32640000000000047}\n",
      "10:19:48 | learner | INFO | ep 47 - performance: {'order_requirements': 2240000, 'container_shortage': 557511, 'operation_number': 4832546}, exploration_params: {'epsilon': 0.3248000000000005}\n",
      "10:19:54 | learner | INFO | ep 48 - performance: {'order_requirements': 2240000, 'container_shortage': 661551, 'operation_number': 4916774}, exploration_params: {'epsilon': 0.3232000000000005}\n",
      "10:20:00 | learner | INFO | ep 49 - performance: {'order_requirements': 2240000, 'container_shortage': 678443, 'operation_number': 4797744}, exploration_params: {'epsilon': 0.3216000000000005}\n",
      "10:20:06 | learner | INFO | ep 50 - performance: {'order_requirements': 2240000, 'container_shortage': 647478, 'operation_number': 4983993}, exploration_params: {'epsilon': 0.3200000000000005}\n",
      "10:20:12 | learner | INFO | ep 51 - performance: {'order_requirements': 2240000, 'container_shortage': 436833, 'operation_number': 4411942}, exploration_params: {'epsilon': 0.3134693877551025}\n",
      "10:20:18 | learner | INFO | ep 52 - performance: {'order_requirements': 2240000, 'container_shortage': 626074, 'operation_number': 4618660}, exploration_params: {'epsilon': 0.30693877551020454}\n",
      "10:20:24 | learner | INFO | ep 53 - performance: {'order_requirements': 2240000, 'container_shortage': 708019, 'operation_number': 4538794}, exploration_params: {'epsilon': 0.30040816326530656}\n",
      "10:20:29 | learner | INFO | ep 54 - performance: {'order_requirements': 2240000, 'container_shortage': 704531, 'operation_number': 5035620}, exploration_params: {'epsilon': 0.2938775510204086}\n",
      "10:20:35 | learner | INFO | ep 55 - performance: {'order_requirements': 2240000, 'container_shortage': 675365, 'operation_number': 4744667}, exploration_params: {'epsilon': 0.2873469387755106}\n",
      "10:20:41 | learner | INFO | ep 56 - performance: {'order_requirements': 2240000, 'container_shortage': 536896, 'operation_number': 4664484}, exploration_params: {'epsilon': 0.2808163265306126}\n",
      "10:20:47 | learner | INFO | ep 57 - performance: {'order_requirements': 2240000, 'container_shortage': 517742, 'operation_number': 4515999}, exploration_params: {'epsilon': 0.27428571428571463}\n",
      "10:20:52 | learner | INFO | ep 58 - performance: {'order_requirements': 2240000, 'container_shortage': 495825, 'operation_number': 4592775}, exploration_params: {'epsilon': 0.26775510204081665}\n",
      "10:20:58 | learner | INFO | ep 59 - performance: {'order_requirements': 2240000, 'container_shortage': 485726, 'operation_number': 4586843}, exploration_params: {'epsilon': 0.26122448979591867}\n",
      "10:21:03 | learner | INFO | ep 60 - performance: {'order_requirements': 2240000, 'container_shortage': 350307, 'operation_number': 4856216}, exploration_params: {'epsilon': 0.2546938775510207}\n",
      "10:21:09 | learner | INFO | ep 61 - performance: {'order_requirements': 2240000, 'container_shortage': 440101, 'operation_number': 4511357}, exploration_params: {'epsilon': 0.24816326530612273}\n",
      "10:21:15 | learner | INFO | ep 62 - performance: {'order_requirements': 2240000, 'container_shortage': 366995, 'operation_number': 4489751}, exploration_params: {'epsilon': 0.24163265306122478}\n",
      "10:21:20 | learner | INFO | ep 63 - performance: {'order_requirements': 2240000, 'container_shortage': 407148, 'operation_number': 4229339}, exploration_params: {'epsilon': 0.23510204081632682}\n",
      "10:21:26 | learner | INFO | ep 64 - performance: {'order_requirements': 2240000, 'container_shortage': 490311, 'operation_number': 4231013}, exploration_params: {'epsilon': 0.22857142857142887}\n",
      "10:21:32 | learner | INFO | ep 65 - performance: {'order_requirements': 2240000, 'container_shortage': 495735, 'operation_number': 4084791}, exploration_params: {'epsilon': 0.22204081632653092}\n",
      "10:21:38 | learner | INFO | ep 66 - performance: {'order_requirements': 2240000, 'container_shortage': 531423, 'operation_number': 4125375}, exploration_params: {'epsilon': 0.21551020408163296}\n",
      "10:21:44 | learner | INFO | ep 67 - performance: {'order_requirements': 2240000, 'container_shortage': 530556, 'operation_number': 4015393}, exploration_params: {'epsilon': 0.208979591836735}\n",
      "10:21:50 | learner | INFO | ep 68 - performance: {'order_requirements': 2240000, 'container_shortage': 271291, 'operation_number': 4631212}, exploration_params: {'epsilon': 0.20244897959183705}\n",
      "10:21:56 | learner | INFO | ep 69 - performance: {'order_requirements': 2240000, 'container_shortage': 355445, 'operation_number': 4429670}, exploration_params: {'epsilon': 0.1959183673469391}\n",
      "10:22:02 | learner | INFO | ep 70 - performance: {'order_requirements': 2240000, 'container_shortage': 358595, 'operation_number': 4645877}, exploration_params: {'epsilon': 0.18938775510204114}\n",
      "10:22:08 | learner | INFO | ep 71 - performance: {'order_requirements': 2240000, 'container_shortage': 269327, 'operation_number': 4763005}, exploration_params: {'epsilon': 0.1828571428571432}\n",
      "10:22:14 | learner | INFO | ep 72 - performance: {'order_requirements': 2240000, 'container_shortage': 252523, 'operation_number': 4390522}, exploration_params: {'epsilon': 0.17632653061224524}\n",
      "10:22:20 | learner | INFO | ep 73 - performance: {'order_requirements': 2240000, 'container_shortage': 224338, 'operation_number': 4651297}, exploration_params: {'epsilon': 0.16979591836734728}\n",
      "10:22:26 | learner | INFO | ep 74 - performance: {'order_requirements': 2240000, 'container_shortage': 419598, 'operation_number': 4577744}, exploration_params: {'epsilon': 0.16326530612244933}\n",
      "10:22:32 | learner | INFO | ep 75 - performance: {'order_requirements': 2240000, 'container_shortage': 319832, 'operation_number': 4523035}, exploration_params: {'epsilon': 0.15673469387755137}\n",
      "10:22:37 | learner | INFO | ep 76 - performance: {'order_requirements': 2240000, 'container_shortage': 291227, 'operation_number': 4569509}, exploration_params: {'epsilon': 0.15020408163265342}\n",
      "10:22:43 | learner | INFO | ep 77 - performance: {'order_requirements': 2240000, 'container_shortage': 235975, 'operation_number': 4438580}, exploration_params: {'epsilon': 0.14367346938775546}\n",
      "10:22:49 | learner | INFO | ep 78 - performance: {'order_requirements': 2240000, 'container_shortage': 209720, 'operation_number': 4472226}, exploration_params: {'epsilon': 0.1371428571428575}\n",
      "10:22:54 | learner | INFO | ep 79 - performance: {'order_requirements': 2240000, 'container_shortage': 203721, 'operation_number': 4475482}, exploration_params: {'epsilon': 0.13061224489795956}\n",
      "10:23:01 | learner | INFO | ep 80 - performance: {'order_requirements': 2240000, 'container_shortage': 215189, 'operation_number': 4290320}, exploration_params: {'epsilon': 0.1240816326530616}\n",
      "10:23:06 | learner | INFO | ep 81 - performance: {'order_requirements': 2240000, 'container_shortage': 203791, 'operation_number': 4321789}, exploration_params: {'epsilon': 0.11755102040816365}\n",
      "10:23:12 | learner | INFO | ep 82 - performance: {'order_requirements': 2240000, 'container_shortage': 230472, 'operation_number': 4216193}, exploration_params: {'epsilon': 0.1110204081632657}\n",
      "10:23:18 | learner | INFO | ep 83 - performance: {'order_requirements': 2240000, 'container_shortage': 148354, 'operation_number': 4274694}, exploration_params: {'epsilon': 0.10448979591836774}\n",
      "10:23:24 | learner | INFO | ep 84 - performance: {'order_requirements': 2240000, 'container_shortage': 196658, 'operation_number': 4234519}, exploration_params: {'epsilon': 0.09795918367346979}\n",
      "10:23:29 | learner | INFO | ep 85 - performance: {'order_requirements': 2240000, 'container_shortage': 117587, 'operation_number': 4409388}, exploration_params: {'epsilon': 0.09142857142857183}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:23:35 | learner | INFO | ep 86 - performance: {'order_requirements': 2240000, 'container_shortage': 129900, 'operation_number': 4370451}, exploration_params: {'epsilon': 0.08489795918367388}\n",
      "10:23:41 | learner | INFO | ep 87 - performance: {'order_requirements': 2240000, 'container_shortage': 150391, 'operation_number': 4372565}, exploration_params: {'epsilon': 0.07836734693877592}\n",
      "10:23:47 | learner | INFO | ep 88 - performance: {'order_requirements': 2240000, 'container_shortage': 240991, 'operation_number': 4263189}, exploration_params: {'epsilon': 0.07183673469387797}\n",
      "10:23:53 | learner | INFO | ep 89 - performance: {'order_requirements': 2240000, 'container_shortage': 108313, 'operation_number': 4400742}, exploration_params: {'epsilon': 0.06530612244898001}\n",
      "10:23:59 | learner | INFO | ep 90 - performance: {'order_requirements': 2240000, 'container_shortage': 153466, 'operation_number': 4211634}, exploration_params: {'epsilon': 0.05877551020408205}\n",
      "10:24:05 | learner | INFO | ep 91 - performance: {'order_requirements': 2240000, 'container_shortage': 144307, 'operation_number': 4295363}, exploration_params: {'epsilon': 0.05224489795918409}\n",
      "10:24:11 | learner | INFO | ep 92 - performance: {'order_requirements': 2240000, 'container_shortage': 135912, 'operation_number': 4270395}, exploration_params: {'epsilon': 0.04571428571428613}\n",
      "10:24:17 | learner | INFO | ep 93 - performance: {'order_requirements': 2240000, 'container_shortage': 130515, 'operation_number': 4309926}, exploration_params: {'epsilon': 0.03918367346938817}\n",
      "10:24:23 | learner | INFO | ep 94 - performance: {'order_requirements': 2240000, 'container_shortage': 129339, 'operation_number': 4209056}, exploration_params: {'epsilon': 0.03265306122449021}\n",
      "10:24:29 | learner | INFO | ep 95 - performance: {'order_requirements': 2240000, 'container_shortage': 134269, 'operation_number': 4254431}, exploration_params: {'epsilon': 0.026122448979592247}\n",
      "10:24:36 | learner | INFO | ep 96 - performance: {'order_requirements': 2240000, 'container_shortage': 104881, 'operation_number': 4244267}, exploration_params: {'epsilon': 0.019591836734694286}\n",
      "10:24:41 | learner | INFO | ep 97 - performance: {'order_requirements': 2240000, 'container_shortage': 89076, 'operation_number': 4300194}, exploration_params: {'epsilon': 0.013061224489796327}\n",
      "10:24:47 | learner | INFO | ep 98 - performance: {'order_requirements': 2240000, 'container_shortage': 94324, 'operation_number': 4267081}, exploration_params: {'epsilon': 0.006530612244898367}\n",
      "10:24:53 | learner | INFO | ep 99 - performance: {'order_requirements': 2240000, 'container_shortage': 88931, 'operation_number': 4267048}, exploration_params: {'epsilon': 4.0766001685454967e-16}\n"
     ]
    }
   ],
   "source": [
    "from maro.simulator import Env\n",
    "from maro.rl import MultiAgentWrapper, TwoPhaseLinearParameterScheduler\n",
    "\n",
    "# Step 1: initialize a CIM environment for a toy dataset. \n",
    "env = Env(\"cim\", \"toy.4p_ssdd_l0.0\", durations=1120)\n",
    "agent_id_list = [str(agent_id) for agent_id in env.agent_idx_list]\n",
    "\n",
    "# Step 2: create DQN agents.\n",
    "agent = MultiAgentWrapper(create_dqn_agents(agent_id_list))\n",
    "\n",
    "# Step 3: Create an actor and a learner to start the training process. \n",
    "scheduler = TwoPhaseLinearParameterScheduler(\n",
    "    max_iter=100,\n",
    "    parameter_names=[\"epsilon\"],\n",
    "    split_ep=50,\n",
    "    start_values=0.4,\n",
    "    mid_values=0.32,\n",
    "    end_values=.0\n",
    ")\n",
    "\n",
    "actor = Actor(env, agent, state_shaper, action_shaper, experience_shaper)\n",
    "learner = Learner(actor, scheduler)\n",
    "learner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('maro': venv)",
   "language": "python",
   "name": "python37764bitmarovenv6353f08d702e48c1950fa6d6d4a4cd49"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}