{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "This notebook demonstrates how to use MARO's reinforcement learning (RL) toolkit to solve the container inventory management ([CIM](https://maro.readthedocs.io/en/latest/scenarios/container_inventory_management.html)) problem. It is formalized as a multi-agent reinforcement learning problem, where each port acts as a decision agent. The agents take actions independently, e.g., loading containers to vessels or discharging containers from vessels.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [State Shaper](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#shapers)\n",
    "\n",
    "State shaper converts the environment observation to the model input state which includes temporal and spatial information. For this scenario, the model input state includes: \n",
    "\n",
    "- Temporal information, including the past week's information of ports and vessels, such as shortage on port and remaining space on vessel. \n",
    "\n",
    "- Spatial information, it including the related downstream port features.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from maro.rl import StateShaper\n",
    "\n",
    "\n",
    "PORT_ATTRIBUTES = [\"empty\", \"full\", \"on_shipper\", \"on_consignee\", \"booking\", \"shortage\", \"fulfillment\"]\n",
    "VESSEL_ATTRIBUTES = [\"empty\", \"full\", \"remaining_space\"]\n",
    "\n",
    "\n",
    "class CIMStateShaper(StateShaper):\n",
    "    def __init__(self, *, look_back, max_ports_downstream):\n",
    "        super().__init__()\n",
    "        self._look_back = look_back\n",
    "        self._max_ports_downstream = max_ports_downstream\n",
    "        self._dim = (look_back + 1) * (max_ports_downstream + 1) * len(PORT_ATTRIBUTES) + len(VESSEL_ATTRIBUTES)\n",
    "\n",
    "    def __call__(self, decision_event, snapshot_list):\n",
    "        tick, port_idx, vessel_idx = decision_event.tick, decision_event.port_idx, decision_event.vessel_idx\n",
    "        ticks = [tick - rt for rt in range(self._look_back - 1)]\n",
    "        future_port_idx_list = snapshot_list[\"vessels\"][tick: vessel_idx: 'future_stop_list'].astype('int')\n",
    "        port_features = snapshot_list[\"ports\"][ticks: [port_idx] + list(future_port_idx_list): PORT_ATTRIBUTES]\n",
    "        vessel_features = snapshot_list[\"vessels\"][tick: vessel_idx: VESSEL_ATTRIBUTES]\n",
    "        state = np.concatenate((port_features, vessel_features))\n",
    "        return str(port_idx), state\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self._dim\n",
    "    \n",
    "# Create a state shaper\n",
    "state_shaper = CIMStateShaper(look_back=7, max_ports_downstream=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Action Shaper](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#shapers)\n",
    "\n",
    "Action shaper is used to convert an agent's model output to an environment executable action. For this specific scenario, the output is a discrete index that corresponds to a percentage indicating the fraction of containers to be loaded to or discharged from the arriving vessel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maro.rl import ActionShaper\n",
    "from maro.simulator.scenarios.cim.common import Action\n",
    "\n",
    "\n",
    "class CIMActionShaper(ActionShaper):\n",
    "    def __init__(self, action_space):\n",
    "        super().__init__()\n",
    "        self._action_space = action_space\n",
    "        self._zero_action_index = action_space.index(0)\n",
    "\n",
    "    def __call__(self, model_action, decision_event, snapshot_list):\n",
    "        assert 0 <= model_action < len(self._action_space)\n",
    "        \n",
    "        scope = decision_event.action_scope\n",
    "        tick = decision_event.tick\n",
    "        port_idx = decision_event.port_idx\n",
    "        vessel_idx = decision_event.vessel_idx\n",
    "        port_empty = snapshot_list[\"ports\"][tick: port_idx: [\"empty\", \"full\", \"on_shipper\", \"on_consignee\"]][0]\n",
    "        vessel_remaining_space = snapshot_list[\"vessels\"][tick: vessel_idx: [\"empty\", \"full\", \"remaining_space\"]][2]\n",
    "        early_discharge = snapshot_list[\"vessels\"][tick:vessel_idx: \"early_discharge\"][0]\n",
    "     \n",
    "        if model_action < self._zero_action_index:\n",
    "            # The number of loaded containers must be less than the vessel's remaining space.\n",
    "            actual_action = max(round(self._action_space[model_action] * port_empty), -vessel_remaining_space)\n",
    "        elif model_action > self._zero_action_index:\n",
    "            # In the case of an early discharge event, we need to subtract the early discharge amount from the expected \n",
    "            # discharge quote.   \n",
    "            plan_action = self._action_space[model_action] * (scope.discharge + early_discharge) - early_discharge\n",
    "            actual_action = round(plan_action) if plan_action > 0 else round(self._action_space[model_action] * scope.discharge)\n",
    "        else:\n",
    "            actual_action = 0\n",
    "\n",
    "        return Action(vessel_idx, port_idx, actual_action)\n",
    "    \n",
    "# Create an action shaper\n",
    "NUM_ACTIONS = 21\n",
    "action_shaper = CIMActionShaper(action_space=list(np.linspace(-1.0, 1.0, NUM_ACTIONS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Experience Shaper](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#shapers)\n",
    "\n",
    "Experience shaper is used to convert an episode trajectory to trainable experiences for RL agents. For this specific scenario, the reward is a linear combination of fulfillment and shortage in a limited time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from maro.rl import ExperienceShaper\n",
    "\n",
    "\n",
    "class TruncatedExperienceShaper(ExperienceShaper):\n",
    "    def __init__(\n",
    "        self, *, time_window: int, time_decay_factor: float, fulfillment_factor: float, shortage_factor: float\n",
    "    ):\n",
    "        super().__init__(reward_func=None)\n",
    "        self._time_window = time_window\n",
    "        self._time_decay_factor = time_decay_factor\n",
    "        self._fulfillment_factor = fulfillment_factor\n",
    "        self._shortage_factor = shortage_factor\n",
    "\n",
    "    def __call__(self, trajectory, snapshot_list):\n",
    "        experiences_by_agent = {}\n",
    "        for i in range(len(trajectory) - 1):\n",
    "            transition = trajectory[i]\n",
    "            agent_id = transition[\"agent_id\"]\n",
    "            if agent_id not in experiences_by_agent:\n",
    "                experiences_by_agent[agent_id] = defaultdict(list)\n",
    "            experiences = experiences_by_agent[agent_id]\n",
    "            experiences[\"state\"].append(transition[\"state\"])\n",
    "            experiences[\"action\"].append(transition[\"action\"])\n",
    "            experiences[\"reward\"].append(self._compute_reward(transition[\"event\"], snapshot_list))\n",
    "            experiences[\"next_state\"].append(trajectory[i + 1][\"state\"])\n",
    "\n",
    "        return experiences_by_agent\n",
    "\n",
    "    def _compute_reward(self, decision_event, snapshot_list):\n",
    "        start_tick = decision_event.tick + 1\n",
    "        end_tick = decision_event.tick + self._time_window\n",
    "        ticks = list(range(start_tick, end_tick))\n",
    "\n",
    "        # calculate tc reward\n",
    "        future_fulfillment = snapshot_list[\"ports\"][ticks::\"fulfillment\"]\n",
    "        future_shortage = snapshot_list[\"ports\"][ticks::\"shortage\"]\n",
    "        decay_list = [\n",
    "            self._time_decay_factor ** i for i in range(end_tick - start_tick)\n",
    "            for _ in range(future_fulfillment.shape[0] // (end_tick - start_tick))\n",
    "        ]\n",
    "\n",
    "        tot_fulfillment = np.dot(future_fulfillment, decay_list)\n",
    "        tot_shortage = np.dot(future_shortage, decay_list)\n",
    "\n",
    "        return np.float32(self._fulfillment_factor * tot_fulfillment - self._shortage_factor * tot_shortage)\n",
    "    \n",
    "# Create an experience shaper\n",
    "experience_shaper = TruncatedExperienceShaper(time_window=100, fulfillment_factor=1.0, shortage_factor=1.0, time_decay_factor=0.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Agent](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#agent)\n",
    "\n",
    "For this scenario, the agent is the abstraction of a port. We choose DQN as our underlying learning algorithm with a TD-error-based sampling mechanism.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maro.rl import AbsAgent, ColumnBasedStore\n",
    "\n",
    "\n",
    "class CIMAgent(AbsAgent):\n",
    "    def __init__(self, name, algorithm, experience_pool: ColumnBasedStore, min_experiences_to_train, num_batches, batch_size):\n",
    "        super().__init__(name, algorithm, experience_pool)\n",
    "        self._min_experiences_to_train = min_experiences_to_train\n",
    "        self._num_batches = num_batches\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "    def train(self):\n",
    "        if len(self._experience_pool) < self._min_experiences_to_train:\n",
    "            return\n",
    "\n",
    "        for _ in range(self._num_batches):\n",
    "            indexes, sample = self._experience_pool.sample_by_key(\"loss\", self._batch_size)\n",
    "            state = np.asarray(sample[\"state\"])\n",
    "            action = np.asarray(sample[\"action\"])\n",
    "            reward = np.asarray(sample[\"reward\"])\n",
    "            next_state = np.asarray(sample[\"next_state\"])\n",
    "            loss = self._algorithm.train(state, action, reward, next_state)\n",
    "            self._experience_pool.update(indexes, {\"loss\": loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Agent Manager](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#agent-manager)\n",
    "\n",
    "The complexities of the environment can be isolated from the learning algorithm by using an AgentManager to manage individual agents. We define a function to create the agents and an agent manager class that implements the ``train`` method where the newly obtained experiences are stored in the agents' experience pools before training, in accordance with the DQN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import yaml\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import smooth_l1_loss\n",
    "from torch.optim import RMSprop\n",
    "\n",
    "from maro.rl import (\n",
    "    ColumnBasedStore, DQN, DQNConfig, FullyConnectedBlock, LearningModuleManager, LearningModule, OptimizerOptions, SimpleAgentManager\n",
    ")\n",
    "from maro.utils import set_seeds\n",
    "\n",
    "\n",
    "def create_dqn_agents(agent_id_list):\n",
    "    set_seeds(1)  # for reproducibility\n",
    "    agent_dict = {}\n",
    "    for agent_id in agent_id_list:\n",
    "        q_module = LearningModule(\n",
    "            \"q_value\",\n",
    "            [FullyConnectedBlock(\n",
    "                input_dim=state_shaper.dim,\n",
    "                hidden_dims=[256, 128, 64],\n",
    "                output_dim=NUM_ACTIONS,\n",
    "                activation=nn.LeakyReLU,\n",
    "                is_head=True,\n",
    "                batch_norm_enabled=True, \n",
    "                softmax_enabled=False,\n",
    "                skip_connection_enabled=False,\n",
    "                dropout_p=.0)\n",
    "            ],\n",
    "            optimizer_options=OptimizerOptions(cls=RMSprop, params={\"lr\": 0.05})\n",
    "        )\n",
    "\n",
    "        algorithm = DQN(\n",
    "            model=LearningModuleManager(q_module),\n",
    "            config=DQNConfig(\n",
    "                reward_decay=.0, \n",
    "                target_update_frequency=5, \n",
    "                tau=0.1, \n",
    "                is_double=True, \n",
    "                per_sample_td_error_enabled=True,\n",
    "                loss_cls=nn.SmoothL1Loss,\n",
    "                num_actions=NUM_ACTIONS\n",
    "            )\n",
    "        )\n",
    "\n",
    "        agent_dict[agent_id] = CIMAgent(\n",
    "            agent_id, algorithm, ColumnBasedStore(), min_experiences_to_train=1024, num_batches=10, batch_size=128\n",
    "        )\n",
    "\n",
    "    return agent_dict\n",
    "\n",
    "\n",
    "class DQNAgentManager(SimpleAgentManager):\n",
    "    def train(self, experiences_by_agent, performance=None):\n",
    "        self._assert_train_mode()\n",
    "\n",
    "        # store experiences for each agent\n",
    "        for agent_id, exp in experiences_by_agent.items():\n",
    "            exp.update({\"loss\": [1e8] * len(list(exp.values())[0])})\n",
    "            self.agent_dict[agent_id].store_experiences(exp)\n",
    "\n",
    "        for agent in self.agent_dict.values():\n",
    "            agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop with [Actor and Learner](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#learner-and-actor)\n",
    "\n",
    "This code cell demonstrates the typical workflow of a learning policy's interaction with a MARO environment. \n",
    "\n",
    "- Initialize an environment with specific scenario and topology parameters. \n",
    "\n",
    "- Define scenario-specific components, e.g. shapers. \n",
    "\n",
    "- Create agents and an agent manager. \n",
    "\n",
    "- Create an actor and a learner to start the training process in which the agent manager interacts with the environment for collecting experiences and updating policies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maro.simulator import Env\n",
    "from maro.rl import AgentManagerMode, SimpleActor, SimpleLearner, TwoPhaseLinearParameterScheduler\n",
    "from maro.utils import LogFormat, Logger\n",
    "\n",
    "# Step 1: initialize a CIM environment for a toy dataset. \n",
    "env = Env(\"cim\", \"toy.4p_ssdd_l0.0\", durations=1120)\n",
    "agent_id_list = [str(agent_id) for agent_id in env.agent_idx_list]\n",
    "\n",
    "# Step 2: create DQN agents and an agent manager to manage them.\n",
    "agent_manager = DQNAgentManager(\n",
    "    name=\"cim_learner\",\n",
    "    mode=AgentManagerMode.TRAIN_INFERENCE,\n",
    "    agent_dict=create_dqn_agents(agent_id_list),\n",
    "    state_shaper=state_shaper,\n",
    "    action_shaper=action_shaper,\n",
    "    experience_shaper=experience_shaper\n",
    ")\n",
    "\n",
    "# Step 3: Create an actor and a learner to start the training process. \n",
    "max_episode = 100\n",
    "scheduler = TwoPhaseLinearParameterScheduler(\n",
    "    max_episode,\n",
    "    parameter_names=[\"epsilon\"],\n",
    "    split_ep=50,\n",
    "    start_values=0.4,\n",
    "    mid_values=0.32,\n",
    "    end_values=.0\n",
    ")\n",
    "\n",
    "actor = SimpleActor(env, agent_manager)\n",
    "learner = SimpleLearner(\n",
    "    agent_manager, actor, scheduler, \n",
    "    logger=Logger(\"single_host_cim_learner\", format_=LogFormat.simple, auto_timestamp=False)\n",
    ")\n",
    "\n",
    "learner.learn()\n",
    "learner.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
