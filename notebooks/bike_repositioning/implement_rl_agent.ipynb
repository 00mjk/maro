{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from maro.rl import AbstractStateShaping\n",
    "from maro.simulator.scenarios.citi_bike import DecisionType, DecisionType, Action\n",
    "\n",
    "class CitiBikeStateShaping(AbstractStateShaping):\n",
    "    def __init__(self, *, relative_tick_list, max_neighbor, station_attribute_list):\n",
    "        super().__init__()\n",
    "        self._relative_tick_list = relative_tick_list\n",
    "        self._station_attribute_list = station_attribute_list\n",
    "        self._max_neighbor = max_neighbor\n",
    "        self._dim = len(relative_tick_list) * (max_neighbor + 1) * len(station_attribute_list) + max_neighbor + 2 # 2: current station idx, current decision event type\n",
    "\n",
    "    def __call__(self, decision_event, snapshot_list):\n",
    "        tick, station_idx, decision_type, action_scope = decision_event.tick, decision_event.station_idx, decision_event.type, decision_event.action_scope\n",
    "        decision_type = 0 if decision_type == DecisionType.Supply else 1 \n",
    "        tick_list = [tick + rt for rt in self._relative_tick_list]\n",
    "        station_idx_list = action_scope.keys()\n",
    "        features = snapshot_list[\"stations\"][tick_list:station_idx_list:self._station_attribute_list]\n",
    "        neighbor_idx_list = [i for i in station_idx_list if i != station_idx]\n",
    "        features = np.concatenate((features, np.array(neighbor_idx_list), np.array([station_idx, decision_type])))\n",
    "\n",
    "        return features.astype(np.float32)\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self._dim\n",
    "    \n",
    "\n",
    "state_shaping = CitiBikeStateShaping(relative_tick_list=[-2, -1, 0], max_neighbor=20,\\\n",
    "                                     station_attribute_list=[\"bikes\", \"shortage\", \"trip_requirement\", \"capacity\", \"weekday\", \n",
    "                                                             \"temperature\", \"weather\", \"holiday\", \"extra_cost\", \"transfer_cost\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from torch.nn.functional import smooth_l1_loss\n",
    "from torch.optim import Adam\n",
    "\n",
    "from maro.rl.models.torch.learning_model import LearningModel\n",
    "from maro.rl.models.torch.mlp_representation import MLPRepresentation\n",
    "from maro.rl.models.torch.q_decision import QDecision\n",
    "from maro.rl.algorithms.torch.dqn import DQN\n",
    "\n",
    "\n",
    "# TODO: env.get_agent_idx_list\n",
    "agent_idx_list = [i for i in range(449)]\n",
    "neighbor_action_dim = 21\n",
    "reposition_action_dim = 21\n",
    "\n",
    "supply_alg_list = []\n",
    "demand_alg_list = []\n",
    "\n",
    "# build algorithm list\n",
    "for idx in agent_idx_list:\n",
    "    shared_representation_layers = MLPRepresentation(name=\"shared_representation\",\\\n",
    "                                                     input_dim=state_shaping.dim,\\\n",
    "                                                     hidden_dims=[state_shaping.dim//2, state_shaping.dim//4],\n",
    "                                                     output_dim=state_shaping.dim//4,\n",
    "                                                     dropout_p=0.1)\n",
    "\n",
    "    supply_neighbor_decision_layers = QDecision(name=\"supply_neighbor_decision\",\\\n",
    "                                                input_dim=shared_representation_layers.output_dim,\n",
    "                                                hidden_dims=None,\n",
    "                                                output_dim=neighbor_action_dim)\n",
    "\n",
    "    demand_neighbor_decision_layers = QDecision(name=\"demand_neighbor_decision\",\\\n",
    "                                                input_dim=shared_representation_layers.output_dim,\n",
    "                                                hidden_dims=None,\n",
    "                                                output_dim=neighbor_action_dim)\n",
    "\n",
    "    supply_reposition_decision_layers = QDecision(name=\"supply_reposition_decision\",\\\n",
    "                                                  input_dim=shared_representation_layers.output_dim,\n",
    "                                                  hidden_dims=None,\n",
    "                                                  output_dim=reposition_action_dim)\n",
    "\n",
    "    demand_reposition_decision_layers = QDecision(name=\"demand_reposition_decision\",\\\n",
    "                                                  input_dim=shared_representation_layers.output_dim,\n",
    "                                                  hidden_dims=None,\n",
    "                                                  output_dim=reposition_action_dim)\n",
    "\n",
    "    # build supply neighbor algorithm\n",
    "    supply_neighbor_learning_model = LearningModel(representation_layers=shared_representation_layers,\n",
    "                                                   decision_layers=supply_neighbor_decision_layers)\n",
    "    supply_neighbor_alg = DQN(num_actions=neighbor_action_dim, gamma=0.9, eval_model=supply_neighbor_learning_model,\\\n",
    "                              loss_func=smooth_l1_loss, optimizer_cls=Adam, optimizer_params={\"lr\":1e-3},\n",
    "                              replace_target_frequency=2)\n",
    "    \n",
    "    # build demand neighbor algorithm\n",
    "    demand_neighbor_learning_model = LearningModel(representation_layers=shared_representation_layers,\n",
    "                                                   decision_layers=demand_neighbor_decision_layers)\n",
    "    demand_neighbor_alg = DQN(num_actions=neighbor_action_dim, gamma=0.9, eval_model=demand_neighbor_learning_model,\\\n",
    "                              loss_func=smooth_l1_loss, optimizer_cls=Adam, optimizer_params={\"lr\":1e-3},\n",
    "                              replace_target_frequency=2)\n",
    "\n",
    "    # build supply reposition algorithm\n",
    "    supply_reposition_learning_model = LearningModel(representation_layers=shared_representation_layers,\n",
    "                                                     decision_layers=supply_reposition_decision_layers)\n",
    "    supply_reposition_alg = DQN(num_actions=reposition_action_dim, gamma=0.9, eval_model=supply_reposition_learning_model,\\\n",
    "                              loss_func=smooth_l1_loss, optimizer_cls=Adam, optimizer_params={\"lr\":1e-3},\n",
    "                              replace_target_frequency=2)\n",
    "\n",
    "    # build demand reposition algorithm\n",
    "    demand_reposition_learning_model = LearningModel(representation_layers=shared_representation_layers,\n",
    "                                                     decision_layers=demand_reposition_decision_layers)\n",
    "    demand_reposition_alg = DQN(num_actions=reposition_action_dim, gamma=0.9, eval_model=demand_reposition_learning_model,\\\n",
    "                              loss_func=smooth_l1_loss, optimizer_cls=Adam, optimizer_params={\"lr\":1e-3},\n",
    "                              replace_target_frequency=2)\n",
    "    \n",
    "    supply_alg_list.append({\n",
    "        \"neighbor\": supply_neighbor_alg,\n",
    "        \"reposition\": supply_reposition_alg\n",
    "    })\n",
    "    demand_alg_list.append({\n",
    "        \"neighbor\": demand_neighbor_alg,\n",
    "        \"reposition\": demand_reposition_alg\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from maro.rl import AbstractActionShaping\n",
    "from maro.simulator.scenarios.citi_bike import DecisionType, Action\n",
    "\n",
    "class CitiBikeActionShaping(AbstractActionShaping):\n",
    "    def __init__(self, action_space: [float]):\n",
    "        super().__init__(action_space)\n",
    "        self._action_space = action_space\n",
    "\n",
    "    def __call__(self, model_action, decision_event) -> Action:\n",
    "        neighbor_action, reposition_action = model_action\n",
    "    \n",
    "        action = None\n",
    "        if neighbor_action == 0 or reposition_action == 0:\n",
    "            return action\n",
    "\n",
    "        if decision_event.type == DecisionType.Supply:\n",
    "            self_bike = decision_event.action_scope[decision_event.station_idx]\n",
    "            target_idx = neighbor_action\n",
    "            target_dock = list(decision_event.action_scope.keys())[target_idx]\n",
    "            available_supplied_bike = min(self_bike, target_dock)\n",
    "            supplied_bike = math.ceil(self._action_space[reposition_action] * available_supplied_bike)\n",
    "            action = Action(decision_event.station_idx, target_idx, supplied_bike)\n",
    "        elif decision_event.type == DecisionType.Demand:\n",
    "            self_dock = decision_event.action_scope[decision_event.station_idx]\n",
    "            target_idx = neighbor_action\n",
    "            target_bike = list(decision_event.action_scope.keys())[target_idx]\n",
    "            available_demanded_bike = min(self_dock, target_bike)\n",
    "            demanded_bike = math.ceil(self._action_space[reposition_action] * available_demanded_bike)\n",
    "            action = Action(target_idx, decision_event.station_idx, demanded_bike)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return action\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return self._action_space\n",
    "    \n",
    "    \n",
    "action_shaping = CitiBikeActionShaping(action_space=[i/(neighbor_action_dim-1) for i in range(neighbor_action_dim)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from maro.rl import AbstractRewardShaping, ReplayBuffer\n",
    "from maro.simulator.scenarios.citi_bike import DecisionEvent\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from maro.rl import AbstractRewardShaping, ReplayBuffer\n",
    "from maro.simulator.scenarios.citi_bike import DecisionEvent\n",
    "\n",
    "\n",
    "class CitiBikeRewardShaping(AbstractRewardShaping):\n",
    "    def __init__(self, fulfillment_factor: float = 1.0, shortage_factor: float = 1.0, cost_factor: float = 0.01,\n",
    "                 time_window: int = 10, time_decay: float = 0.97):\n",
    "        super().__init__()\n",
    "        self._fulfillment_factor = fulfillment_factor\n",
    "        self._shortage_factor = shortage_factor\n",
    "        self._cost_factor = cost_factor\n",
    "        self._time_window = time_window\n",
    "        self._time_decay = time_decay\n",
    "\n",
    "    def __call__(self, snapshot_list, replay_buffer: ReplayBuffer, state_shaping):\n",
    "        for i in range(replay_buffer.size):\n",
    "            decision_type = replay_buffer.get(i, \"decision_type\")\n",
    "            station_idx = replay_buffer.get(i, \"station_idx\")\n",
    "            action_scope = replay_buffer.get(i, \"action_scope\")\n",
    "            neighbor_idxs = [ne for ne in action_scope.keys() if ne != station_idx]\n",
    "\n",
    "            start_tick = replay_buffer.get(i, \"tick\")\n",
    "            end_tick = start_tick + self._time_window\n",
    "\n",
    "            ticks = list(range(start_tick, end_tick))\n",
    "            # broadcast\n",
    "            global_decay_list = [self._time_decay ** i for i in range(self._time_window)\n",
    "                                 for _ in range(449)]\n",
    "            local_decay_list = [self._time_decay ** i for i in range(self._time_window)\n",
    "                                for _ in range(len(neighbor_idxs))]\n",
    "\n",
    "            global_fulfillment = np.dot(snapshot_list[\"stations\"][ticks::\"fulfillment\"], global_decay_list)\n",
    "            local_fulfillment = np.dot(snapshot_list[\"stations\"][ticks:neighbor_idxs:\"fulfillment\"], local_decay_list)\n",
    "\n",
    "            global_shortage = np.dot(snapshot_list[\"stations\"][ticks::\"shortage\"], global_decay_list)\n",
    "            local_shortage = np.dot(snapshot_list[\"stations\"][ticks:neighbor_idxs:\"shortage\"], local_decay_list)\n",
    "\n",
    "            global_cost = np.dot(snapshot_list[\"stations\"][ticks::\"transfer_cost\"], global_decay_list)\n",
    "            local_cost = np.dot(snapshot_list[\"stations\"][ticks:neighbor_idxs:\"transfer_cost\"], local_decay_list)\n",
    "\n",
    "            replay_buffer.put(i, \"decision_type\", decision_type)\n",
    "            replay_buffer.put(i, \"global_reward\", np.float32(self._fulfillment_factor * global_fulfillment -\n",
    "                                                             self._shortage_factor * global_shortage -\n",
    "                                                             self._cost_factor * global_cost))\n",
    "\n",
    "            replay_buffer.put(i, \"local_reward\", np.float32(self._fulfillment_factor * local_fulfillment -\n",
    "                                                            self._shortage_factor * local_shortage -\n",
    "                                                            self._cost_factor * local_cost))\n",
    "\n",
    "            hack_decision_event = DecisionEvent(station_idx=station_idx, tick=end_tick,\n",
    "                                                frame_index=end_tick, action_scope_func=None,\n",
    "                                                decision_type=decision_type)\n",
    "            hack_decision_event._action_scope = action_scope\n",
    "\n",
    "            replay_buffer.put(i, \"next_state\", state_shaping(hack_decision_event, snapshot_list))\n",
    "\n",
    "                  \n",
    "reward_shaping = CitiBikeRewardShaping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maro.utils import ExperiencePool, ExperiencePoolType\n",
    "from maro.rl import ReplayBuffer\n",
    "from maro.simulator.scenarios.citi_bike import DecisionType\n",
    "\n",
    "class CitiBikeReplayBuffer(ReplayBuffer):\n",
    "    def __init__(self, experience_pool: ExperiencePool):\n",
    "        super().__init__(experience_pool)\n",
    "\n",
    "    def sample_from_experience_pool(self, batch_size, is_local=True):\n",
    "        neighbor_idx_list = self._experience_store.apply_multi_samplers(\n",
    "            category_samplers=[(\"neighbor_td_error\", [(lambda i, o: (i, o), batch_size)])])[\"neighbor_td_error\"]\n",
    "        reposition_idx_list = self._experience_store.apply_multi_samplers(\n",
    "            category_samplers=[(\"reposition_td_error\", [(lambda i, o: (i, o), batch_size)])])[\"reposition_td_error\"]\n",
    "\n",
    "        if is_local:\n",
    "            neighbor_sample_dict = self._experience_store.get(category_idx_batches=[\n",
    "                ('state', neighbor_idx_list),\n",
    "                ('local_reward', neighbor_idx_list),\n",
    "                ('neighbor_action', neighbor_idx_list),\n",
    "                ('next_state', neighbor_idx_list)\n",
    "            ])\n",
    "            reposition_sample_dict = self._experience_store.get(category_idx_batches=[\n",
    "                ('state', reposition_idx_list),\n",
    "                ('local_reward', reposition_idx_list),\n",
    "                ('reposition_action', reposition_idx_list),\n",
    "                ('next_state', reposition_idx_list)\n",
    "            ])\n",
    "        else:\n",
    "            neighbor_sample_dict = self._experience_store.get(category_idx_batches=[\n",
    "                ('state', neighbor_idx_list),\n",
    "                ('global_reward', neighbor_idx_list),\n",
    "                ('neighbor_action', neighbor_idx_list),\n",
    "                ('next_state', neighbor_idx_list)\n",
    "            ])\n",
    "            reposition_sample_dict = self._experience_store.get(category_idx_batches=[\n",
    "                ('state', reposition_idx_list),\n",
    "                ('global_reward', reposition_idx_list),\n",
    "                ('reposition_action', reposition_idx_list),\n",
    "                ('next_state', reposition_idx_list)\n",
    "            ])\n",
    "\n",
    "        neighbor_res_dict = {}\n",
    "        for key in neighbor_sample_dict.keys():\n",
    "            if key == \"local_reward\" or key == \"global_reward\":\n",
    "                neighbor_res_dict[\"reward\"] = neighbor_sample_dict[key]\n",
    "            elif key == \"neighbor_action\":\n",
    "                neighbor_res_dict[\"action\"] = neighbor_sample_dict[key]\n",
    "            else:\n",
    "                neighbor_res_dict[key] = neighbor_sample_dict[key]\n",
    "                \n",
    "        reposition_res_dict = {}\n",
    "        for key in reposition_sample_dict.keys():\n",
    "            if key == \"local_reward\" or key == \"global_reward\":\n",
    "                reposition_res_dict[\"reward\"] = reposition_sample_dict[key]\n",
    "            elif key == \"reposition_action\":\n",
    "                reposition_res_dict[\"action\"] = reposition_sample_dict[key]\n",
    "            else:\n",
    "                reposition_res_dict[key] = reposition_sample_dict[key]\n",
    "\n",
    "        for k in neighbor_res_dict.keys():\n",
    "            neighbor_res_dict[k] = np.asarray(neighbor_res_dict[k])\n",
    "\n",
    "        for k in reposition_res_dict.keys():\n",
    "            reposition_res_dict[k] = np.asarray(reposition_res_dict[k])\n",
    "\n",
    "        return neighbor_idx_list, neighbor_res_dict, reposition_idx_list, reposition_res_dict\n",
    "\n",
    "supply_experience_pool_list = [ExperiencePool(ExperiencePoolType.FixedSize, 20000, \"random\") for _ in agent_idx_list]\n",
    "supply_replay_buffer_list = [CitiBikeReplayBuffer(supply_experience_pool_list[idx]) for idx in agent_idx_list]\n",
    "demand_experience_pool_list = [ExperiencePool(ExperiencePoolType.FixedSize, 20000, \"random\") for _ in agent_idx_list]\n",
    "demand_replay_buffer_list = [CitiBikeReplayBuffer(demand_experience_pool_list[idx]) for idx in agent_idx_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maro.rl import Agent, AgentMode, TrainingLoopHyperparameters\n",
    "from maro.simulator.scenarios.citi_bike import DecisionEvent\n",
    "\n",
    "class CitiBikeAgent(Agent):\n",
    "    def __init__(self, name, algorithm, replay_buffer, training_loop_hyperparams,\n",
    "                 mode: AgentMode = AgentMode.TRAIN_INFERENCE, state_shaping=None, action_shaping=None,\n",
    "                 reward_shaping=None):\n",
    "        super().__init__(name, algorithm, replay_buffer, training_loop_hyperparams, mode, state_shaping,\n",
    "                         action_shaping, reward_shaping)\n",
    "\n",
    "    def choose_action(self, decision_event: DecisionEvent, snapshot_list, exploration_rate=0):\n",
    "        self._assert_inference_mode()\n",
    "        model_state = self._state_shaping(decision_event, snapshot_list)\n",
    "        neighbor_action = self._algorithm[\"neighbor\"].choose_action(model_state, exploration_rate)\n",
    "        reposition_action = self._algorithm[\"reposition\"].choose_action(model_state, exploration_rate)\n",
    "        env_action = self._action_shaping((neighbor_action, reposition_action), decision_event)\n",
    "\n",
    "        self._current_transition = {\"state\": model_state,\n",
    "                                    \"next_state\": None,\n",
    "                                    \"neighbor_action\": neighbor_action,\n",
    "                                    \"reposition_action\": reposition_action,\n",
    "                                    \"tick\": decision_event.tick,\n",
    "                                    \"neighbor_td_error\": 1e7,\n",
    "                                    \"reposition_td_error\": 1e7,\n",
    "                                    \"local_reward\": None,\n",
    "                                    \"global_reward\": None,\n",
    "                                    \"decision_type\": decision_event.type,\n",
    "                                    \"station_idx\": decision_event.station_idx,\n",
    "                                    \"action_scope\": decision_event.action_scope,\n",
    "                                    }\n",
    "\n",
    "        return env_action\n",
    "\n",
    "    def postprocess(self, snapshot_list):\n",
    "        self._assert_inference_mode()\n",
    "        if self._reward_shaping is not None:\n",
    "            self._reward_shaping(snapshot_list, self._trajectory, self._state_shaping)\n",
    "\n",
    "    def train(self):\n",
    "        self._assert_train_mode()\n",
    "        if self._trajectory.num_experiences < self._min_experiences_to_train:\n",
    "            return\n",
    "\n",
    "        for _ in range(self._num_steps):\n",
    "            neighbor_idx_list, neighbor_bath, reposition_idx_list, reposition_bath = self._trajectory.sample_from_experience_pool(\n",
    "                self._batch_size)\n",
    "\n",
    "            neighbor_loss = self._algorithm[\"neighbor\"].train_on_batch(neighbor_bath)\n",
    "            self._trajectory.update_experience_pool(neighbor_idx_list, \"neighbor_td_error\", neighbor_loss)\n",
    "            reposition_loss = self._algorithm[\"reposition\"].train_on_batch(reposition_bath)\n",
    "            self._trajectory.update_experience_pool(reposition_idx_list, \"reposition_td_error\", reposition_loss)\n",
    "\n",
    "    \n",
    "training_loop_hyperparams = TrainingLoopHyperparameters(num_steps=10, batch_size=128, min_experiences_to_train=1024)\n",
    "\n",
    "supply_agent_dict = {}\n",
    "demand_agent_dict = {}\n",
    "agent_idx_list = [i for i in range(449)]\n",
    "for agent_idx in agent_idx_list :\n",
    "    supply_agent_dict[agent_idx] = CitiBikeAgent(name=agent_idx,\n",
    "                                                 algorithm=supply_alg_list[agent_idx],\n",
    "                                                 replay_buffer=supply_replay_buffer_list[agent_idx],\n",
    "                                                 training_loop_hyperparams=training_loop_hyperparams,\n",
    "                                                 mode=AgentMode.TRAIN_INFERENCE,\n",
    "                                                 state_shaping=state_shaping,\n",
    "                                                 action_shaping=action_shaping,\n",
    "                                                 reward_shaping=reward_shaping)\n",
    "\n",
    "    demand_agent_dict[agent_idx] = CitiBikeAgent(name=agent_idx,\n",
    "                                                 algorithm=demand_alg_list[agent_idx],\n",
    "                                                 replay_buffer=demand_replay_buffer_list[agent_idx],\n",
    "                                                 training_loop_hyperparams=training_loop_hyperparams,\n",
    "                                                 mode=AgentMode.TRAIN_INFERENCE,\n",
    "                                                 state_shaping=state_shaping,\n",
    "                                                 action_shaping=action_shaping,\n",
    "                                                 reward_shaping=reward_shaping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 17.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep-0 env run time: 0.4954318086306254 min\n",
      "ep-0 epsilon: 0.396\n",
      "ep-0 total decision number: 2159\n",
      "ep-0 total shortage: 2204.0\n",
      "ep-0 total fulfillment: 50631.0\n",
      "ep-0 total trip requirement: 52835.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "449it [00:06, 70.14it/s] \n",
      "2it [00:00, 15.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep-1 env run time: 0.48678418397903445 min\n",
      "ep-1 epsilon: 0.392\n",
      "ep-1 total decision number: 2101\n",
      "ep-1 total shortage: 2391.0\n",
      "ep-1 total fulfillment: 50444.0\n",
      "ep-1 total trip requirement: 52835.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "449it [00:05, 84.40it/s] \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep-2 env run time: 0.5125433882077535 min\n",
      "ep-2 epsilon: 0.388\n",
      "ep-2 total decision number: 2210\n",
      "ep-2 total shortage: 2505.0\n",
      "ep-2 total fulfillment: 50330.0\n",
      "ep-2 total trip requirement: 52835.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "449it [00:05, 78.18it/s] \n"
     ]
    }
   ],
   "source": [
    "# training phase\n",
    "max_train_ep = 100\n",
    "initial_train_seed = 1024\n",
    "max_eps = 0.4\n",
    "\n",
    "import time\n",
    "from random import randint\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from maro.simulator import Env\n",
    "from maro.simulator.scenarios.citi_bike.common import Action, DecisionEvent, DecisionType\n",
    "\n",
    "# 1st step: init CitiBike environment, tick unit is minute, resolution unit is tick\n",
    "train_env = Env(scenario=\"citi_bike\", topology=\"arthur_test\", start_tick=0, durations=3600, snapshot_resolution=10)\n",
    "test_env = Env(scenario=\"citi_bike\", topology=\"arthur_test\", start_tick=0, durations=3600, snapshot_resolution=10)\n",
    "\n",
    "def exploration_schedule(ep, max_eps, max_train_ep):\n",
    "    step = max_eps/max_train_ep\n",
    "    return max_eps - (ep + 1) * step\n",
    "\n",
    "for ep in range(max_train_ep):\n",
    "    is_done: bool = False\n",
    "    reward: int = None\n",
    "    decision_event: DecisionEvent = None\n",
    "    action: Action = None\n",
    "    decision_count = 0\n",
    "    epsilon = exploration_schedule(ep, max_eps, max_train_ep)\n",
    "    \n",
    "    env_start = time.time()\n",
    "    reward, decision_event, is_done = train_env.step(action)\n",
    "    while not is_done:\n",
    "        if decision_event.type == DecisionType.Supply:\n",
    "            cur_agent = supply_agent_dict[decision_event.station_idx]\n",
    "        elif decision_event.type == DecisionType.Demand:\n",
    "            cur_agent = demand_agent_dict[decision_event.station_idx]\n",
    "\n",
    "        action = cur_agent.choose_action(decision_event, train_env.snapshot_list, epsilon)\n",
    "        reward, decision_event, is_done = train_env.step(action)\n",
    "        cur_agent.on_env_feedback(reward, train_env.snapshot_list)\n",
    "        decision_count += 1\n",
    "        \n",
    "    env_end = time.time()\n",
    "    print(f\"ep-{ep} env run time: {(env_end-env_start)/60} min\")\n",
    "    print(f\"ep-{ep} epsilon: {epsilon}\")\n",
    "    print(f\"ep-{ep} total decision number: {decision_count}\")\n",
    "    no_action_total_shortage = train_env.snapshot_list[\"stations\"][::\"shortage\"].sum()\n",
    "    print(f\"ep-{ep} total shortage: {no_action_total_shortage}\")\n",
    "    no_action_total_fulfillment = train_env.snapshot_list[\"stations\"][::\"fulfillment\"].sum()\n",
    "    print(f\"ep-{ep} total fulfillment: {no_action_total_fulfillment}\")\n",
    "    no_action_total_trip_requirement = train_env.snapshot_list[\"stations\"][::\"trip_requirement\"].sum()\n",
    "    print(f\"ep-{ep} total trip requirement: {no_action_total_trip_requirement}\")\n",
    "    \n",
    "    for supply_agent, demand_agent in tqdm(zip(supply_agent_dict.values(), demand_agent_dict.values())):\n",
    "        supply_train_start = time.time()\n",
    "        supply_agent.postprocess(train_env.snapshot_list)\n",
    "        supply_agent.flush_replay_buffer()\n",
    "        supply_agent.train()\n",
    "        supply_train_end = time.time()\n",
    "        # print(f\"ep-{ep} supply agent {supply_agent._agent_name} train time: {(supply_train_end-supply_train_start)/60} min\")\n",
    "        demand_train_start = time.time()\n",
    "        demand_agent.postprocess(train_env.snapshot_list)\n",
    "        demand_agent.flush_replay_buffer()\n",
    "        demand_agent.train()\n",
    "        demand_train_end = time.time()\n",
    "        # print(f\"ep-{ep} demand agent {demand_agent._agent_name} train time: {(demand_train_end-demand_train_start)/60} min\")\n",
    "\n",
    "    train_env.reset()\n",
    "\n",
    "\n",
    "# Test\n",
    "is_done: bool = False\n",
    "reward: int = None\n",
    "decision_event: DecisionEvent = None\n",
    "action: Action = None\n",
    "decision_count = 0\n",
    "epsilon = 0\n",
    "\n",
    "reward, decision_event, is_done = test_env.step(action)\n",
    "while not is_done:\n",
    "    if decision_event.type == DecisionType.Supply:\n",
    "        cur_agent = supply_agent_dict[decision_event.station_idx]\n",
    "    elif decision_event.type == DecisionType.Demand:\n",
    "        cur_agent = demand_agent_dict[decision_event.station_idx]\n",
    "\n",
    "    action = cur_agent.choose_action(decision_event, test_env.snapshot_list, epsilon)\n",
    "    reward, decision_event, is_done = train_env.step(action)\n",
    "    cur_agent.on_env_feedback(reward, train_env.snapshot_list)\n",
    "    decision_count += 1\n",
    "\n",
    "print(f\"test phase total decision number: {decision_count}\")\n",
    "no_action_total_shortage = test_env.snapshot_list[\"stations\"][::\"shortage\"].sum()\n",
    "print(f\"test phase total shortage: {no_action_total_shortage}\")\n",
    "no_action_total_fulfillment = test_env.snapshot_list[\"stations\"][::\"fulfillment\"].sum()\n",
    "print(f\"test phase total fulfillment: {no_action_total_fulfillment}\")\n",
    "no_action_total_trip_requirement = test_env.snapshot_list[\"stations\"][::\"trip_requirement\"].sum()\n",
    "print(f\"test phase total trip requirement: {no_action_total_trip_requirement}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
